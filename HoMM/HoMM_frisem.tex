\documentclass{beamer}
\setbeamerfont{subsection in toc}{size=\footnotesize}
%\setbeameroption{show notes on second screen=left} %enable for notes
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{hyperref}
\lstset{language=python,frame=single}
\usepackage{verbatim}
\usepackage[longnamesfirst]{natbib}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{relsize}
\usepackage{appendixnumberbeamer}
\usepackage{xparse}
\usepackage{multimedia}
\usepackage{xcolor}
\usepackage[normalem]{ulem}
\usepackage{hyperref}
\usepackage{tikz}
\usetikzlibrary{matrix,backgrounds}
\usetikzlibrary{positioning}
\usetikzlibrary{shapes,arrows}
\usetikzlibrary{positioning}

\captionsetup[subfigure]{labelformat=empty}

\tikzset{onslide/.code args={<#1>#2}{%
  \only<#1>{\pgfkeysalso{#2}} 
}}

\tikzstyle{block} = [rectangle, draw, thick, align=center, rounded corners]
\tikzstyle{boundingbox} = [thick, lightgray]
\tikzstyle{dashblock} = [rectangle, draw, thick, align=center, dashed]
\tikzstyle{conc} = [ellipse, draw, thick, dashed, align=center]
\tikzstyle{netnode} = [circle, draw, very thick, inner sep=0pt, minimum size=0.5cm]
\tikzstyle{relunode} = [rectangle, draw, very thick, inner sep=0pt, minimum size=0.5cm]
\tikzstyle{line} = [draw, very thick, -latex']
\tikzstyle{arrow} = [draw, ->, very thick]

\definecolor{bpurp}{HTML}{984ea3}
\definecolor{bblue}{HTML}{377eb8}

\usetheme[numbering=none]{metropolis}

\begin{document}

\title{Meta-Mapping: Toward human-like flexibility from deep learning}
\author{Andrew Lampinen}
\date{}
\frame{\titlepage}

\begin{frame}{Humans are good at adapting flexibly}
\centering
\includegraphics[width=\textwidth]{figures/poker.png}
\end{frame}

\begin{frame}{Deep learning is (usually) not}
\begin{columns}
\begin{column}{0.5\textwidth}
Deep learning generally requires lots of data to achieve good performance on a task.\uncover<2->{ However, there are a few exceptions:}
\begin{itemize}
    \item<3-> Lots of new work on meta-learning (learning to learn), but this means learning from little data, not none.
    \item<4-> Some language work I'll come back to later.
\end{itemize}
\end{column}

\begin{column}{0.5\textwidth}
\includegraphics[width=\textwidth]{figures/frostbite.jpg}
\end{column}
\end{columns}
\end{frame}

\begin{frame}[standout]
How can we build deep learning models that can perform a new task on their first try?
\end{frame}

\section{Meta-mapping}

\begin{frame}{Tasks as functions}
\begin{columns}
\begin{column}{0.5\textwidth}
It's useful to think of tasks or behaviors as functions mapping input to output:
\begin{itemize}
    \item Poker hand \(\rightarrow\) bet
    \item Chess position \(\rightarrow\) move
    \item Object \(\rightarrow\) classification
\end{itemize}
Standard deep learning (and meta-learning) try to infer this function from examples.
\end{column}

\begin{column}{0.5\textwidth}
\includegraphics[width=\textwidth]{figures/poker_function.png}
\end{column}
\end{columns}
\end{frame}

%\begin{frame}[standout]
%It's useful to think of a task/behavior as a function that maps some inputs to some outputs.
%\end{frame}

\begin{frame}{Functional flexibility}
If tasks are functions, altering the task = adapting the function. 
\begin{columns}
\begin{column}{0.5\textwidth}
\vspace{2em}
\includegraphics[width=\textwidth]{figures/poker.png}
\end{column}
\begin{column}{0.5\textwidth}
\vspace{2em}
\only<1>{
\includegraphics[width=\textwidth]{figures/poker_function.png}
}
\only<2>{
\includegraphics[width=\textwidth]{figures/lose_poker_function.png}
}
\end{column}
\end{columns}
\end{frame}

\begin{frame}{Meta-mappings}
How do we get from a task function to an adapted one?
\begin{itemize}
\item We propose \textbf{meta-mappings}, higher-order functions which transform functions into other functions.
\end{itemize}
\includegraphics[width=\textwidth]{figures/meta_mapping_poker.png}
\end{frame}

\begin{frame}[standout]
We model flexibility as the ability to transform basic task functions, via a meta-mapping. 
\end{frame}

\begin{frame}{Meta-mappings are analogous to basic tasks}
\begin{columns}
\begin{column}{0.5\textwidth}
\vspace{2em}
\includegraphics[width=\textwidth]{figures/poker_function.png}
\end{column}
\begin{column}{0.5\textwidth}
\vspace{2em}
\includegraphics[width=\textwidth]{figures/try_to_lose_function.png}
\end{column}
\end{columns}
\begin{itemize}[<+->]
\item Both are just functions (they just have different types of inputs and outputs).
\item This means we can apply all the tricks we know about how to learn functions to learning meta-mappings.
\item ... Assuming we have a function representation to learn over.
\end{itemize}
\end{frame}

\begin{frame}[standout]
There is a functional analogy between basic tasks and meta-mappings.
\end{frame}

\section{An architecture for representing tasks as functions}

\begin{frame}{Architecture preview}
Our goal is to implement:
\begin{itemize}
\item A way of representing tasks as functions.
\item A way of representing those tasks/functions as vectors.
\item A way of learning to transform these task-vectors to perform meta-mappings.
\end{itemize}
\end{frame}

\begin{frame}{Task-specific computations are low-dimensional and abstract}
* Figure here of CNN architecture, talk about pretraining and tuning just last layer.
\end{frame}

\begin{frame}{A function-oriented meta-learning architecture}
* figure here, showing panel a from the paper schematic.
\end{frame}

\begin{frame}[standout]
We propose a meta-learning architecture which generates task/function-embeddings and uses them to parameterize (part of) a deep network which executes the task. % shorten this
\end{frame}

\section{Homoiconic meta-mapping}

\begin{frame}{Meta-mapping as transforming function embeddings}
* figure here, showing panels a and c from the paper schematic, with a black box in between.
\end{frame}

\begin{frame}{Meta-mapping from examples}
* figure here, repeat figure from meta-mappings are analogous part, replace task reps with embeddings 
\end{frame}

\begin{frame}{Homoiconic meta-mapping architecture}
* figure here, showing full paper schematic (first mask a and c, then show). 
* parsimonious, homoiconic, etc.
\end{frame}

\begin{frame}[standout]
Both basic tasks and meta-mappings can be viewed as functions inferred from examples, so we can use the same approaches and networks for both.
\end{frame}

\begin{frame}{Interim summary}
\begin{itemize}
\item Basic tasks are functions from inputs to outputs (e.g. poker hands to bets), which can be inferred from examples.
\item We represent the task specific computations as relatively simple transformations of an embedding space, parameterized from a task embedding.
\item One type of flexibility is systematically altering these tasks (e.g. trying to bet on losing hands instead of winning ones).
\item This can be seen as a \textbf{meta-mapping}, that is, a function which takes tasks as input and produces tasks as output (e.g. maps try-to-win-at-poker to try-to-lose-at-poker).
\item We implement the meta-mapping function as a transformation of the task embeddings.
\item This function can be learned and implemented using exactly the same networks as the basic tasks.
\end{itemize}
\end{frame}

\section{Experiments}

\begin{frame}{Experiments overview}
This same approach can be used in many different settings: 
\begin{table}
\center
\begin{tabular}{|c|c|c|}
\hline
Experiment & Type & Cognitive motivation \\
\hline
Polynomials & Regression & Proof of concept \\
Card games & Regression & Adapting strategies \\
Categories & Classification & Adapting concepts \\
Grid worlds & RL & Adapting behaviors \\
\hline
\end{tabular}
\end{table}
\end{frame}

\end{document}
