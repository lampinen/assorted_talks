\documentclass{beamer}
\setbeamerfont{subsection in toc}{size=\footnotesize}
%\setbeameroption{show notes on second screen=left} %enable for notes
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{hyperref}
\lstset{language=python,frame=single}
\usepackage{verbatim}
\usepackage[longnamesfirst]{natbib}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{relsize}
\usepackage{appendixnumberbeamer}
\usepackage{xparse}
\usepackage{multimedia}
\usepackage{xcolor}
\usepackage[normalem]{ulem}
\usepackage{hyperref}
\usepackage{tikz}
\usetikzlibrary{matrix,backgrounds}
\usetikzlibrary{positioning}
\usetikzlibrary{shapes,arrows}
\usetikzlibrary{positioning}

\captionsetup[subfigure]{labelformat=empty}

\tikzset{onslide/.code args={<#1>#2}{%
  \only<#1>{\pgfkeysalso{#2}} 
}}

\tikzstyle{block} = [rectangle, draw, thick, align=center, rounded corners]
\tikzstyle{boundingbox} = [thick, lightgray]
\tikzstyle{dashblock} = [rectangle, draw, thick, align=center, dashed]
\tikzstyle{conc} = [ellipse, draw, thick, dashed, align=center]
\tikzstyle{netnode} = [circle, draw, very thick, inner sep=0pt, minimum size=0.5cm]
\tikzstyle{relunode} = [rectangle, draw, very thick, inner sep=0pt, minimum size=0.5cm]
\tikzstyle{line} = [draw, very thick, -latex']
\tikzstyle{arrow} = [draw, ->, very thick]

\definecolor{bpurp}{HTML}{984ea3}
\definecolor{bblue}{HTML}{377eb8}

\usetheme[numbering=none]{metropolis}

\begin{document}

\title{Meta-Mapping: Toward human-like flexibility from deep learning}
\author{Andrew Lampinen}
\date{}
\frame{\titlepage}

\begin{frame}{Humans are good at adapting flexibly}
*Figure of "try to lose" goes here
*Possibly also categorization example?
\end{frame}

\begin{frame}{Deep learning is (usually) not}
* Systems are very good at doing what they have lots of data on.
* Lots of new work on meta-learning (learning to learn), but this means learning from little data, not none.
* Some prior zero-shot learning work, which I'll come back to later.
\end{frame}

\begin{frame}[standout]
How can we build deep learning models that are as flexible as humans?
\end{frame}

\section{Meta-mapping}

\begin{frame}{Tasks as functions}
* We can think of tasks or behaviors as functions mapping input to output
    * Board position -> move
    * Object -> classification
    * Poker hand -> bet
* This is just the standard end-to-end deep learning framework.
\end{frame}

\begin{frame}[standout]
It's useful to think of a task/behavior as a function that maps some inputs to some outputs.
\end{frame}

\begin{frame}{Functional flexibility}
* If tasks are functions, altering the task = changing the function being computed *in a systematic way.*
* We propose \textbf{meta-mappings}, which transform functions into other functions.
* For example, we could have a try to lose meta-mapping, which would e.g. map "try to win at poker" to "try to lose at poker."
\end{frame}

\begin{frame}{Meta-mappings are analogous to basic tasks}
* Meta-mappings are higher-order functions, which take functions (=tasks/behaviors) as input and produce functions (=tasks/behaviors) as output.
* (figure?) Can be learned from examples, e.g. figure out what ``losing'' means from comparisons of winning and losing different games.
* That is, learning a meta-mapping can be framed as an end-to-end learning problem.
* ... as long as you're given a representation for functions to learn over.
\end{frame}

\begin{frame}[standout]
We model flexibility as the ability to transform one function (=task=behavior) to a systematically related one.
\end{frame}

\section{An architecture for representing tasks as functions}

\begin{frame}{Abstract stuff ahead}
The next two sections of the talk will be somewhat technical and abstract. To understand the rest of the talk afterward, all you really need to know is that we're trying to (parsimoniously) implement:
\begin{itemize}
\item A way of representing tasks as functions.
\item A way of representing those tasks/functions as vectors.
\item A way of learning to transform these task-vectors to perform meta-mappings.
\end{itemize}
\end{frame}

\begin{frame}{Task-specific computations are low-dimensional and abstract}
* Figure here of CNN architecture, talk about pretraining and tuning just last layer.
\end{frame}

\begin{frame}{A function-oriented meta-learning architecture}
* figure here, showing panel a from the paper schematic.
\end{frame}

\begin{frame}[standout]
We propose a meta-learning architecture which generates task/function-embeddings and uses them to parameterize (part of) a deep network which executes the task. % shorten this
\end{frame}

\section{Homoiconic meta-mapping}

\begin{frame}{Meta-mapping as transforming function embeddings}
* figure here, showing panels a and c from the paper schematic, with a black box in between.
\end{frame}

\begin{frame}{Meta-mapping from examples}
* figure here, repeat figure from meta-mappings are analogous part, replace task reps with embeddings 
\end{frame}

\begin{frame}{Homoiconic meta-mapping architecture}
* figure here, showing full paper schematic (first mask a and c, then show). 
* parsimonious, homoiconic, etc.
\end{frame}

\end{document}
